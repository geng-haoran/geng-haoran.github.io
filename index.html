<!DOCTYPE HTML>
<html lang="en">

<!-- head -->
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-66DNLPJ6PY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-66DNLPJ6PY');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Haoran Geng   |  ËÄøÊµ©ÁÑ∂</title>
  
  <meta name="author" content="Haoran Geng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/icon.png">
</head>

<!-- bib hide -->
<script type="text/javascript">
  function hideshow(which){
  if (!document.getElementById)
  return
  if (which.style.display=="block")
  which.style.display="none"
  else
  which.style.display="block"
  }
</script>

<!-- body -->
<body>
  <!-- self-intro -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:73%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haoran Geng | ËÄøÊµ©ÁÑ∂</name>
              </p>
              <p>
              <intro>I am a junior student in 
                <a href="https://cfcs.pku.edu.cn/english/research/turing_program/introduction1/index.htm" target="_blank">
                  <d>Turing Class</d></a> 
                at the <a href="https://eecs.pku.edu.cn/" target="_blank">
                  <d>School of Electronic Engineering and Computer Science(EECS)</d></a>,
                   <a href="https://english.pku.edu.cn/" target="_blank"><d>Peking University</d></a> with GPA ranking <strong>1</strong>/95. I'm also a research visitor at <a href="https://www.stanford.edu/" target="_blank"><d>Stanford University</d></a>
                and a research intern at <a href="https://bigai.ai/" target="_blank"><d>Beijing Institute for General Artificial Intelligence (BIGAI)</d></a>. 
                I am honored to be advised by Prof. <a href="https://hughw19.github.io/" target="_blank">
                  <d>He Wang</d></a>,  Prof. <a href="https://geometry.stanford.edu/member/guibas/" target="_blank">Leonidas J. Guibas</a> and Dr.
                  <a href="https://siyuanhuang.com/" target="_blank"><d>Siyuan Huang</d></a>. 
                  In addition, I am privileged to work closely with 
                Prof. <a href="https://ericyi.github.io/" target="_blank"><d>Li Yi</d></a>, 
                Prof. <a href="https://zsdonghao.github.io/" target="_blank"><d>Hao Dong</d></a>,
                Prof. <a href="https://www.yangyaodong.com/" target="_blank"><d>Yaodong Yang</d></a>
                  and  Prof. <a href="https://cfcs.pku.edu.cn/baoquan/" target="_blank"><d>Baoquan Chen</d></a>.
                  I am also grateful to have grown up and studied with my twin brother <a class="a1" href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>, which has been a truly unique and special experience for me.
                
                </intro>
              <p style="text-align:center">
                <a href="mailto:ghr@stu.pku.edu.cn" target="_blank">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Inr-6rEAAAAJ" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/geng-haoran/" target="_blank">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/HaoranGeng2/" target="_blank">Twitter</a>&nbsp/&nbsp
                <a href="images/WeChat.jpg" target="_blank">WeChat</a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="images/face.jpg" class="hoverZoomLink">
              <a href="https://hits.seeyoufarm.com" target="_blank"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgeng-haoran.github.io&count_bg=%23FF8400&title_bg=%23545353&icon=tableau.svg&icon_color=%23F3F209&title=hits&edge_flat=false"/></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    </tr>
  </tbody></table>

  <!-- News -->
  <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    <br>
    <heading>News</heading>
    <br>
    <td style="padding:0px;width:100%;vertical-align:middle">
    <p>
      <li>[2023/07] üéâ Two papers get accepted to ICCV 2023.</li>
      <li>[2023/07] üéâ One paper gets accepted to Machine Learning Journal.</li>
      <li>[2023/07] We will present <em>UniDexGrasp++</em> at RSS 2023 @ <a href="https://learn-dex-hand.github.io/rss2023/" target="_blank">Learning Dexterous Manipulation</a> on July 14, 2023</li>
      <li>[2023/06] Welcome to check out our posters for <a href="https://cvpr.thecvf.com/virtual/2023/poster/22552" target="_blank">GAPartNet</a>, 
        <a href="https://cvpr.thecvf.com/virtual/2023/poster/22553" target="_blank">PartManip</a> and 
        <a href="https://cvpr.thecvf.com/virtual/2023/poster/21614" target="_blank">UniDexGrasp</a> at CVPR 2023 on June 20, 2023</li>
      <li>[2023/06] I will present <em>GAPartNet</em> at CVPR 2023 @ <a href="https://struco3d.github.io/cvpr2023/" target="_blank">StruCo3D</a> on June 18, 2023 </li>
      <li>[2023/06] I will present <em>Learning Part-Aware Visual Actionable Affordance for 3D Articulated Object Manipulation</em> at CVPR 2023 @ <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">3DVR</a> on June 18, 2023</li>
      <li>[2023/03] üéâ GAPartNet is selected as a <b><font color='red'>highlight</font></b> at CVPR 2023 (Top 10% of accepted papers, top 2.5% of submissions).</li>
      <li>[2023/02] üéâ Three papers get accepted to CVPR 2023 with GAPartNet receiving final reviews of <b><font color='red'>all accepts (the highest ratings)</font></b>.</li>
      <li>[2023/01] üéâ One paper gets accepted to ICRA 2023.</li>
    </p> 
  </td>
  </tr>
  </tbody></table>

  <!-- Research -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <br>
    <heading>Research</heading>
    <br>
    <p>
      My research interest is broadly in <strong>3D Computer Vision</strong> and <strong>Robotics</strong>, with particular
    interests in generalizable object perception, understanding and manipulation currently. My research objective
    is to build an intelligent agent with the robust and generalizable ability to perceive and interact
    with a complex real-world environment.
    </p>

    <!-- UniDexGrasp++ -->
    <tr>
      <td style="padding:20px;width:30%;vertical-align:middle">
        <br>
        <br>
        <div class="one">
          <div class="two" id='malle_image'>
            <br>
            <img src='images/UniDexGrasp++large.png' width="200"></div>
          <br>
          <img src='images/UniDexGrasp++large.png' width="200">
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }

          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
        <a href="" target="_blank">
          <papertitle><br>UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning</papertitle>
        </a>
        <br>
        <a class="a2" href="https://wkwan7.github.io/" target="_blank">Weikang Wan</a>*,
        <a:focus><strong>Haoran Geng*</strong></a:focus>,
        <a:focus>Yun Liu</a:focus>,
        <a:focus>Zikang Shan</a:focus>,
        <a class="a2" href="https://www.yangyaodong.com/" target="_blank">Yaodong Yang</a>,
        <a class="a2" href="https://ericyi.github.io/" target="_blank">Li Yi</a>,
        <a class="a2" href="https://hughw19.github.io/" target="_blank">He Wang</a><span>&#8224;</span>
        <br>
        (*equal contribution)
        <br>
        <a href="https://arxiv.org/abs/2304.00464" target="_blank">ArXiv</a>
        /
        <a href="https://pku-epic.github.io/UniDexGrasp++/" target="_blank">Project Page</a>
        /
        <a href="https://github.com/PKU-EPIC/UniDexGrasp2">Code</a>
        /
        <a href="javascript:hideshow(document.getElementById('unidexgrasp++'))">Bibtex</a>
        <p id="unidexgrasp++" style="font:1px; display: none">
          @article{wan2023unidexgrasp++,
            <br>
            &emsp;title={UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning},
            <br>
            &emsp;author={Wan, Weikang and Geng, Haoran and Liu, Yun and Shan, Zikang and Yang, Yaodong and Yi, Li and Wang, He},
            <br>
            &emsp;journal={arXiv preprint arXiv:2304.00464},
            <br>
            &emsp;year={2023}
          }
        </p>
        <br>
        <em><strong>ICCV 2023</strong></em>
        <!-- <br> -->
        <!-- <em><strong>RSS 2023 @ Learning Dexterous Manipulation, <b><font color='red'>Spotlight Presentation</font></b></strong></em> -->
        <p></p>
        <p> We propose a novel, object-agnostic method for learning a universal policy for dexterous object 
          grasping from realistic point cloud observations and proprioceptive information under a table-top setting.</p>
      </td>
    </tr>

    <!-- ARNOLD -->
    <tr>
     <td style="padding:20px;width:30%;vertical-align:middle">
       <div class="one">
         <div class="two" id='malle_image'>
           <br>
           <img src='images/arnold.gif' width="190"></div>
         <br>
         <img src='images/arnold.gif' width="190">
       </div>
       <script type="text/javascript">
         function malle_start() {
           document.getElementById('malle_image').style.opacity = "1";
         }
         function malle_stop() {
           document.getElementById('malle_image').style.opacity = "0";
         }
         malle_stop()
       </script>
     </td>
     <td style="padding:20px;width:70%;vertical-align:middle">
       <a href="https://arnold-benchmark.github.io/" target="_blank">
         <papertitle><br>ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes</papertitle>
       </a>
       <br>
       <a class="a2" href="https://nikepupu.github.io/" target="_blank">Ran Gong</a>*,
       <a class="a2" href="https://huangjy-pku.github.io/" target="_blank">Jiangyong Huang</a>*,
       <a class="a2" href="https://www.zyz.lol/app/aboutme/aboutme.html" target="_blank">Yizhou Zhao</a>,
       <a:focus><strong>Haoran Geng</strong></a:focus>,
       <a class="a2" href="https://xfgao.github.io/" target="_blank">Xiaofeng Gao</a>,
       <a class="a2" href="https://qywu.github.io/" target="_blank">Qingyang Wu</a>,
       <a class="a2" href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>,
       <a class="a2" href="https://www.linkedin.com/in/josephziheng/" target="_blank">Ziheng Zhou</a>,
       <a class="a2" href="http://web.cs.ucla.edu/~dt/" target="_blank">Demetri Terzopoulos</a>,
       <a class="a2" href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>,
       <a class="a2" href="https://buzz-beater.github.io/" target="_blank">Baoxiong Jia</a>,
       <a class="a2" href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>,
       <br>
       <a href="http://arxiv.org/abs/2304.04321" target="_blank">ArXiv</a>
       /
       <a href="https://arnold-benchmark.github.io/" target="_blank">Project Page</a>
       /
       <a href="https://github.com/arnold-benchmark/arnold" target="_blank">Code</a>
       /
       <a href="javascript:hideshow(document.getElementById('ARNOLD'))">Bibtex</a>
       
       <!-- <pre> -->
         <p id="ARNOLD" style="font:1px; display: none">
         @article{gong2023arnold,
           <br>
           &emsp;title={ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes},
           <br>
           &emsp;author={Gong, Ran and Huang, Jiangyong and Zhao, Yizhou and Geng, Haoran and Gao, Xiaofeng and Wu, Qingyang and Ai, Wensi and Zhou, Ziheng and Terzopoulos, Demetri and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
           <br>
           &emsp;journal={arXiv preprint arXiv:2304.04321},
           <br>
           &emsp;year={2023}
           <br>
         }
     </p >
       <br>
       <em><strong>ICCV 2023</strong></em>
       <br>
       <em><strong>CoRL 2022 @ LangRob</strong>, <b><font color='red'>Spotlight Presentation</font></b></em>
       <br>
       <!-- <em>Under Review</font></b></em> -->
       <p></p>
       <p>We present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. 
       </p>
     </td>
    </tr>

    <!-- ReDMan -->
    <tr>
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image'>
            <br>
            <img src='images/ReDMan.png' width="190"></div>
            <br>
          <img src='images/ReDMan.png' width="190">
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }

          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
        <a href="pdf/RedMan.pdf">
          <papertitle>ReDMan: Reliable Dexterous Manipulation with Safe Reinforcement Learning</papertitle>
        </a>
        <br>
        <a class="a2", href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>*,
        <a:focus>Jiaming Ji*</a:focus>, 
        <a class="a2" href="https://cypypccpy.github.io/" target="_blank">Yuanpei Chen</a>*,
        <a:focus><strong>Haoran Geng</strong></a:focus>,
        <a:focus>Fangwei Zhong</a:focus>,
        <a class="a2" href="https://www.yangyaodong.com/" target="_blank">Yaodong Yang</a><span>&#8224;</span>,
        <br>
        <a href="pdf/RedMan.pdf" target="_blank">Paper</a>
        /
        <a href="https://github.com/OmniSafeAI/ReDMan" target="_blank">Code</a>
        <br>
        <em><strong>Machine Learning (Journal)</strong></em>
        <p></p>
        <p>
          We introduce ReDMan, an open-source simulation platform that provides a standardized implementation of safe RL algorithms for Reliable Dexterous Manipulation. 
        </p>
      </td>
    </tr>

    <!-- GAPartNet -->
    <tr>
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image'>
            <br>
            <img src='images/GAPartNet_gif.gif' width="190"></div>
          <br>
          <img src='images/GAPartNet_gif.gif' width="190">
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }

          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
        <a href="https://pku-epic.github.io/GAPartNet/" target="_blank">
          <papertitle><br>GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts</papertitle>
        </a>
        <br>
        <a:focus><strong>Haoran Geng*</strong></a:focus>,
        <a class="a2" href="https://helinxu.github.io/" target="_blank">Helin Xu</a>*,
        <a:focus>Chengyang Zhao</a:focus>*,
        <a class="a2" href="https://chaoxu.xyz/#bio" target="_blank">Chao Xu</a>,
        <a class="a2" href="https://ericyi.github.io/" target="_blank">Li Yi</a>,
        <a class="a2" href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>,
        <a class="a2" href="https://hughw19.github.io/" target="_blank">He Wang</a><span>&#8224;</span>
        <br>
        <a href="https://arxiv.org/abs/2211.05272" target="_blank">ArXiv</a>
        /
        <a href="https://pku-epic.github.io/GAPartNet/" target="_blank">Project Page</a>
        /
        <a href="https://github.com/PKU-EPIC/GAPartNet" target="_blank">Code</a>
        /
        <a href="https://forms.gle/3qzv8z5vP2BT5ARN7" target="_blank">Dataset</a>
        /
        <a href="https://pku-epic.github.io/GAPartNet/images/poster.pdf" target="_blank">Poster</a>
        /
        <a href="https://cvpr.thecvf.com/virtual/2023/poster/22552" target="_blank">CVPR Page</a>
        /
        <a href="https://mp.weixin.qq.com/s/elWJx4wMzNtORQUlACOQ6w" target="_blank">Media(CFCS)</a>
        /
        <a href="javascript:hideshow(document.getElementById('GApartNet'))">Bibtex</a>
        
        <!-- <pre> -->
        <p id="GApartNet" style="font:1px; display: none">
          @article{geng2022gapartnet,
            <br>
            &emsp;title={GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts},
            <br>
            &emsp;author={Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
            <br>
            &emsp;journal={arXiv preprint arXiv:2211.05272},
            <br>
            &emsp;year={2022}
            <br>
        }
        </p>
      <!-- </pre> -->
        <br>
        <em><strong>CVPR 2023</strong>, <b><font color='red'>Highlight</font></b> (Top <b><font color='red'>2.5%</font></b> of submissions) with <b><font color='red'>all top ratings</font></b> </em>
        <p></p>
        <p>We propose to learn cross-category generalizable object perception and manipulation skills via Generalizable 
          and Actionable Parts(GAPart), and present GAPartNet, a large-scale interactive dataset with rich part annotations.
        </p>
      </td>
    </tr>
     
    <!-- PartManip -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image'>
            <img src='images/PartManip_half2.png' width="194"></div>
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }
          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <p></p>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://pku-epic.github.io/PartManip/" target="_blank">
          <papertitle>PartManip: Learning Cross-Category Generalizable Part Manipulation Policy
            from Point Cloud Observations</papertitle>
        </a>
        <br>
        <a:focus><strong>Haoran Geng*</strong></a:focus>,
        <a:focus>Ziming Li</a:focus>*,
        <a class="a2", href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>,
        <a class="a2" href="https://jychen18.github.io/" target="_blank">Jiayi Chen</a>,
        <a class="a2" href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a>,
        <a class="a2" href="https://hughw19.github.io/" target="_blank">He Wang</a><span>&#8224;</span>
        <br>
        <a href="https://arxiv.org/abs/2303.16958" target="_blank">ArXiv</a>
        /
        <a href="https://pku-epic.github.io/PartManip/" target="_blank">Project Page</a>
        /
        <a href="https://github.com/PKU-EPIC/PartManip" target="_blank">Code</a>
        /
        <a href="https://forms.gle/DqdPvLE6pNWZf2XR8" target="_blank">Dataset</a>
        /
        <a href="pdf/PartManip Poster_final.pdf" target="_blank">Poster</a>
        /
        <a href="https://cvpr.thecvf.com/virtual/2023/poster/22553" target="_blank">CVPR Page</a>
        /
        <a href="javascript:hideshow(document.getElementById('PartManip'))">Bibtex</a>
        
        <!-- <pre> -->
          <p id="PartManip" style="font:1px; display: none">
          @article{geng2023partmanip,
            <br>
            &emsp;title={PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations},
            <br>
            &emsp;author={Geng, Haoran and Li, Ziming and Geng, Yiran and Chen, Jiayi and Dong, Hao and Wang, He},
            <br>
            &emsp;journal={arXiv preprint arXiv:2303.16958},
            <br>
            &emsp;year={2023}
            <br>
          }
      </p >
        <br>
        <em><strong>CVPR 2023</strong></em>
        <p></p>
        
        <p>We introduce a large-scale, cross-category part-based object manipulation benchmark 
          with tasks in realistic, vision-based settings and design a novel augmented state-to-vision 
          distillation method for these challenging tasks. </p>
      </td>
    </tr>

    <!-- UniDexGrasp -->
    <tr>
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image'>
            <br>
            <img src='images/uni_half.png' width="190"></div>
            <br>
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }

          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
        <a href="https://pku-epic.github.io/UniDexGrasp/" target="_blank">
          <papertitle><br>UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</papertitle>
        </a>
        <br>
        <a class="a2" href="https://xyz-99.github.io/" target="_blank">Yinzhen Xu</a>*,
        <a class="a2" href="https://wkwan7.github.io/" target="_blank">Weikang Wan</a>*,
        <a:focus>Jialiang Zhang*</a:focus>,
        <a:focus>Haoran Liu*</a:focus>,
        <a:focus>Zikang Shan</a:focus>,
        <a:focus>Hao Shen</a:focus>,
        <a:focus>Ruicheng Wang</a:focus>,
        <a:focus><strong>Haoran Geng</strong></a:focus>,
        <a class="a2" href="https://yijiaweng.github.io/" target="_blank">Yijia Weng</a>,
        <a class="a2" href="https://jychen18.github.io/" target="_blank">Jiayi Chen</a>,
        <a class="a2" href="https://tengyu.ai/" target="_blank">Tengyu Liu</a>,
        <a class="a2" href="https://ericyi.github.io/" target="_blank">Li Yi</a>,
        <a class="a2" href="https://hughw19.github.io/" target="_blank">He Wang</a><span>&#8224;</span>
        <br>
        <a href="https://arxiv.org/abs/2303.00938" target="_blank">ArXiv</a>
        /
        <a href="https://pku-epic.github.io/UniDexGrasp/" target="_blank">Project Page</a>
        /
        <a href="https://github.com/PKU-EPIC/UniDexGrasp" target="_blank">Code</a>
        /
        <a href="https://cvpr.thecvf.com/virtual/2023/poster/21614" target="_blank">CVPR Page</a>
        /
        <a href="javascript:hideshow(document.getElementById('UniDexGrasp'))">Bibtex</a>
        <!-- <pre> -->
          <p id="UniDexGrasp" style="font:1px; display: none">
            @article{xu2023unidexgrasp,
            <br>
            &emsp;title={UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy},
            <br>
            &emsp;author={Xu, Yinzhen and Wan, Weikang and Zhang, Jialiang and Liu, Haoran and Shan, Zikang and Shen, Hao and Wang, Ruicheng and Geng, Haoran and Weng, Yijia and Chen, Jiayi and others},
            <br>
            &emsp;journal={arXiv preprint arXiv:2303.00938},
            <br>
            &emsp;year={2023}
            <br>
            }
      </p >
        <br>
        <em><strong>CVPR 2023</strong></em>
        <p></p>
        <p> We tackle the problem of learning universal robotic dexterous grasping from a 
          point cloud observation under a table-top setting. </p>
      </td>
    </tr>

    <!-- RLAfford -->
    <tr>
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image'>
            <br>
            <img src='images/E2E_3.png' width="190"></div>
            <br>
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }

          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
        <a href="https://sites.google.com/view/rlafford/" target="_blank">
          <papertitle><br>RLAfford: End-to-End Affordance Learning for Robotic Manipulation</papertitle>
        </a>
        <br>
        <a class="a2", href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>*,
        <a class="a2" href="https://boshi-an.github.io/" target="_blank">Boshi An</a>*,
        <a:focus><strong>Haoran Geng</strong></a:focus>,
        <a class="a2" href="https://cypypccpy.github.io/" target="_blank">Yuanpei Chen</a>,
        <a class="a2" href="https://www.yangyaodong.com/" target="_blank">Yaodong Yang</a><span>&#8224;</span>,
        <a class="a2" href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><span>&#8224;</span>
        <br>
        <a href="https://arxiv.org/abs/2209.12941" target="_blank">ArXiv</a>
        /
        <a href="https://sites.google.com/view/rlafford/" target="_blank">Project Page</a>
        /
        <a href="https://www.bilibili.com/video/BV1cM411b7ZD/?spm_id_from=444.41.list.card_archive.click" target="_blank">Video</a>
        /
        <a href="https://github.com/hyperplane-lab/RLAfford" target="_blank">Code</a>
        /
        <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498641&idx=1&sn=06a3f2a314e43def32df5ee76e47fa55&chksm=fb1af784cc6d7e92852a53e994d57139d7beed0e379bf3ea7c5d66e4fe558b4a70d14f19832d&mpshare=1&scene=1&srcid=0125YlFVj1P4UMAt6V9CcKKq&sharer_sharetime=1674658986915&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd"
        target="_blank">Media (CFCS)</a>
        /
        <a href="javascript:hideshow(document.getElementById('rlafford'))">Bibtex</a>
          <p id="rlafford" style="font:1px; display: none">
            @article{geng2022end,
            <br>
            &emsp;title={End-to-End Affordance Learning for Robotic Manipulation},
            <br>
            &emsp;author={Geng, Yiran and An, Boshi and Geng, Haoran and Chen, Yuanpei and Yang, Yaodong and Dong, Hao},
            <br>
            &emsp;journal={International Conference on Robotics and Automation (ICRA)},
            <br>
            &emsp;year={2023}
            <br>
            }
      </p >
        <br>
        <em><strong>ICRA 2023</strong></em>
        <p></p>
        <p>In this study, we take advantage of visual affordance by using the contact information generated during the RL training process to predict contact maps of interest. </p>
      </td>
    </tr>

    <!-- PLAfford -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image'>
            <img src='images/PLAfford.jpg' width="194"></div>
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }
          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <p></p>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="" target="_blank">
          <papertitle>Learning Part-Aware Visual Actionable Affordance for 3D Articulated Object Manipulation</papertitle>
        </a>
        <br>
        <a:focus> Yuanchen Ju*</a>,
        <a:focus><strong>Haoran Geng*</strong></a:focus>,
        <a:focus>Ming Yang</a:focus>*,
        <a class="a2" href="https://jychen18.github.io/" target="_blank">Jiayi Chen</a>,
        <a class="a2", href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>,
        <a:focus>Yaroslav Ponomarenko</a:focus>,
        <a:focus>Taewhan Kim</a:focus>,
        <a class="a2" href="https://hughw19.github.io/" target="_blank">He Wang</a>,
        <a class="a2" href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><span>&#8224;</span>
        <br>
        <a href="">Paper</a>
        /
        <a href="https://drive.google.com/file/d/19bPWbLLnsQTBU5xT64yRQ6HgbOjrZoKv/view?usp=sharing" target="_blank">Video</a>
        /
        <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">Workshop</a>
        
        <!-- <pre> -->
          <p id="PartManip" style="font:1px; display: none">
          @article{geng2023partmanip,
            <br>
            &emsp;title={PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations},
            <br>
            &emsp;author={Geng, Haoran and Li, Ziming and Geng, Yiran and Chen, Jiayi and Dong, Hao and Wang, He},
            <br>
            &emsp;journal={arXiv preprint arXiv:2303.16958},
            <br>
            &emsp;year={2023}
            <br>
          }
      </p >
        <br>
        <em><strong>CVPR 2023 @ 3DVR</strong>, <b><font color='red'>Spotlight Presentation</font></b></em>
        <p></p>
        
        <p>We introduces Part-aware Affordance Learning methods. Our approach first learns a part 
          prior, subsequently generating an affordance map. We further enhance precision by 
          introducing a part-level scoring system, designed to identify the best part for manipulation. </p>
      </td>
    </tr>

  </tbody></table>

  <!-- Preprints -->
  <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <br>
    <heading>Preprints</heading>
    <br>

  </tbody></table> -->

  <!-- Before 2022 -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <br>
    <heading>Before 2022</heading>
    <br>
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image'>
            <br>
            <img src='images/YINGCAI2.png' width="190"></div>
            <br>
        </div>
        <script type="text/javascript">
          function malle_start() {
            document.getElementById('malle_image').style.opacity = "1";
          }
          function malle_stop() {
            document.getElementById('malle_image').style.opacity = "0";
          }
          malle_stop()
        </script>
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
        <a href="pdf/YINGCAI.pdf" target="_blank">
          <papertitle>Ministry of Education Talent Program Thesis (Physics Track)</papertitle>
        </a>
        <br>
        <a:focus><strong>Haoran Geng</strong></a:focus>, 
        <a class="a2", href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>, 
        <a:focus>Xintian Dong</a:focus>,
        <a:focus>Yue Meng</a:focus>,
        <a:focus>Xujv Sun</a:focus>,
        <a:focus>Houpu Niu</a:focus>
        <br>
        <a href=pdf/YINGCAI.pdf target="_blank">PDF</a>
        <br>
        <!-- <em>Selected as the Outstanding Thesis of the National Physics Forum 2018</em> -->
        <p>
          I was selected for the Ministry of Education Talent Program and conducted physics research at Nankai University during my high school years.
      </td>
  </tbody></table>

  <!-- Recent Talks -->
  <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <br>
          <heading>Recent Talks</heading>
        <br>
        <td style="padding:0px;width:100%;vertical-align:middle">
          <P>
            <li>I presented <em>Towards Generalizable Object Perception and Manipulation</em> at <a href="https://struco3d.github.io/cvpr2023/" target="_blank">Stanford UGVR</a> Research Salon on July 9, 2023.
              <br>
              <a href="https://drive.google.com/file/d/1hfktvO5A2P89nd09zvnqIxmBa282MpEO/view?usp=sharing" target="_blank">Slides</a>
            </li>
            <li>I presented <em>GAPartNet</em> at CVPR 2023 @ <a href="https://struco3d.github.io/cvpr2023/" target="_blank">StruCo3D</a> on June 18, 2023.
              <br>
              <a href="https://drive.google.com/file/d/1n-KZzTjg7J_AW2uxd_buxFd0CVsph02u/view?usp=sharing" target="_blank">Video</a>
              /
              <a href="pdf/GAPartNet_poster.pdf" target="_blank">Poster</a>
            </li>
            <li>I presented <em>Learning Part-Aware Visual Actionable Affordance for 3D Articulated Object Manipulation</em> at 
              CVPR 2023 @ <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">3DVR</a> on June 18, 2023.
              <br>
              <a href="https://drive.google.com/file/d/19bPWbLLnsQTBU5xT64yRQ6HgbOjrZoKv/view?usp=sharing" target="_blank">Video</a>
            </li>
            <li>I presented <em>Generalizable Cross-Category Object Perception and Manipulation</em> at 
              <a href="http://dev3.noahlab.com.hk/" target="_blank"> Noah's Ark Lab, Huawei</a> on June 2, 2023.
              <br>
            </li>
            <li>I presented <em>PartManip: Learning Cross-Category Generalizable Part Manipulation 
              Policy from Point Cloud Observations</em> at 
              <a href="https://mp.weixin.qq.com/s/jx8lbVnEugQSciXJ_j4VSQ" target="_blank">Turing 
                Student Research Forum</a> on May 27, 2023 and was 
                honored to receive the <strong>Best Presentation Award</strong> and <strong>Best Poster Award</strong>.
              <br>
              <a href="https://mp.weixin.qq.com/s/jx8lbVnEugQSciXJ_j4VSQ" target="_blank">Media</a>
              /
              <a href="pdf/PartManip_poster.pdf" target="_blank">Poster</a>
            </li>
            <li>I presented <em>Generalizable Part-based Cross-Category Object Perception and Manipulation</em>
              at <a href="https://mp.weixin.qq.com/s/zun5HXDwY6b79UJhhOxkIA" target="_blank">CVPR2023 Seminar</a> (hosted by 
              SenseTime) on May 28, 2023.
              <br>
              <a href="https://mp.weixin.qq.com/s/zun5HXDwY6b79UJhhOxkIA" target="_blank">Media</a>
              /
              <a href="pdf/GAPartNet_poster.pdf" target="_blank">Poster</a>
            </li>
            <li>I presented <em>GAPartNet: Cross-Category Domain-Generalizable Object Perception and 
              Manipulation via Generalizable and Actionable Parts</em> at 
              <a href="https://mp.weixin.qq.com/s/Whnkr9yPa4VXk4XflQL5Uw" target="_blank">Scientific 
                Research Exhibition</a> (hosted by School of EECS, PKU) on May 13, 2023 and was 
                honored to receive the <strong>Best Presentation Award</strong>.
              <br>
              <a href="https://mp.weixin.qq.com/s/Whnkr9yPa4VXk4XflQL5Uw" target="_blank">Media</a>
              /
              <a href="pdf/GAPartNet_poster.pdf" target="_blank">Poster</a>
            </li>
            <li>I presented <em>GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts</em> at <a href="http://www.csig3dv.net/" target="_blank">China3DV 2023</a> on April 21, 2023, which was selected as a <strong>Fastforward Report</strong> (<strong>11</strong>/75).
              <br>
              <a href="http://www.csig3dv.net/ShowPoster.html" target="_blank">Media</a>
              /
              <a href="images/GAPartNet_China3DV_poster.jpg" target="_blank">Poster</a>
              /
              <a href="video/GAPartNet_China3DV.mp4" target="_blank">Video</a>
            </li>
            <li>I presented <em>Learning Generalizable and Actionable Parts for Cross-Category Object Perception and Manipulation</em> at the <a href="https://cfcs.pku.edu.cn/news/240724.htm" target="_blank">Turing Student Research Forum</a> at CFCS, Peking University on June 8, 2022 and was honored to receive the <strong>Outstanding Presentation Award</strong>.
              <br>
              <a href="https://www.bilibili.com/video/BV1Rv4y1376L?p=10" target="_blank">Video</a>
              /
              <a href="https://cfcs.pku.edu.cn/news/240724.htm" target="_blank">Media</a>
            </li>
          </P>
        </td>

  </tbody></table>
    
  <!-- Services -->
  <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <br>
        <heading>Services</heading>
        <br>
      <td style="padding:0px;width:100%;vertical-align:middle">
        <p>
          <li>Reviewer: CVPR2023, ICCV2023, NIPS2023, RSS2023</li>
        </p>
        <p>
          <li>I serve as the chair of <a href="https://mp.weixin.qq.com/s/jx8lbVnEugQSciXJ_j4VSQ" target="_blank">Turing Student Research Forum 2023</a>.</li>
        </p>
        <p>
          <li>One of the leaders of <a href="https://lcpu.club/wiki/index.php?title=%E9%A6%96%E9%A1%B5" target="_blank">Linux Club of Peking University(LCPU)</a></li>
        </p>
        <p>
          <li>We organized <a href="https://hpcgame.pku.edu.cn/" target="_blank">High Performance Computing Integrated Competitiveness Competition</a> and I was also the manager of the AI part.</li>
        </p>
      </td>
    </tr>
  </tbody></table>

  <!-- Experience -->
  <table width="100%" align="center" border="0" cellpadding="10"><tbody>
      <br>
          <heading>Experience</heading>
      <br>
      <tr>
        <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/Stanford.png", width="105"></td>
        <td width="100%" valign="center">
          <strong><a href="https://www.stanford.edu/" target="_blank"><papertitle>Stanford University</papertitle> </a></strong>
          <br> <em>2023.02 - Present</em><br>  <strong>Visiting Research Student through the </strong><a href="https://engineering.stanford.edu/students-academics/programs/global-engineering-programs/chinese-ugvr/" target="_blank">UGVR Program </a> 
          <!-- <br> <em>2023.06 - 2023.09 (expected)</em> <br> <strong>Student of</strong> <a href="https://engineering.stanford.edu/students-academics/programs/global-engineering-programs/chinese-ugvr/">UGVR Program </a>  -->
          <br> Research Advisor: Prof. <a href="https://geometry.stanford.edu/member/guibas/" target="_blank">Leonidas J. Guibas</a>
        </td>
      </tr>  
      <tr>
        <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/Bigai.png", width="108"></td>
        <td width="90%" valign="center">
          <strong><a href="https://www.bigai.ai/" target="_blank"><papertitle>Beijing Institute for General Artificial Intelligence (BIGAI)</papertitle></a></strong>
          <br> <em>2021.12 - Present</em>
          <br> <strong>Research Intern</strong>
          <br> Research Advisor: <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a> 
          <br> Academic Advisor: Prof. <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>
        </td>
      </tr>
      <tr>
        <td style="padding-left:27px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/vcl-logo.png", width="90"></td>
        <td width="90%" valign="center">
          <strong><a href="http://vcl.pku.edu.cn/index.html" target="_blank"><papertitle>Visual Computing and Learning Lab(VCL)</papertitle></a></strong>
          <br> <em>2022.6 - 2022.9</em>
          <br> <strong>Summer Research Intern</strong>
          <br> Research Advisor: Prof. <a href="https://hughw19.github.io/" target="_blank">He Wang</a> 
          <br> Academic Advisor: Prof. <a href="https://cfcs.pku.edu.cn/baoquan/" target="_blank">Baoquan Chen</a>
        </td>
      </tr>
      <tr>
          <br>
          <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/PKU.png", width="108"></td>
          <td width="90%" valign="center">
            <strong><a href="https://english.pku.edu.cn/" target="_blank"><papertitle>Peking University (PKU)</papertitle></a></strong>
            <br> <em>2020.09 - Present</em>
            <br> <strong> Undergraduate Student</strong>, <a href="https://cfcs.pku.edu.cn/english/research/turing_program/introduction1/index.htm" target="_blank">Turing Class</a>
            <br> GPA ranking:  <strong>1</strong>/95
            <br> Research Advisor: Prof. <a href="https://hughw19.github.io/" target="_blank">He Wang</a>
            <!-- <br> Academic Advisor: Prof. <a href="http://www.liweiwang-pku.com/">Liwei Wang</a> -->
          </td>
      </tr>
      
        
  </tbody></table>

  <!-- Selected Awards and Honors -->
  <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <br>
          <heading>Selected Awards and Honors</heading>

          <br>
        <td style="padding:0px;width:100%;vertical-align:middle">
          
          <p>
            <li>2023: Turing Student Research Forum: Best Presentation Award (3000RMB¬•)</li>
          </p>
          <p>
            <li>2023: Turing Student Research Forum: Best Poster Award (1000RMB¬•)</li>
          </p>
          <p>
            <li>2023: School of EECS Research Exhibition: Best Presentation Award (3000RMB¬•)</li>
          </p>
          <p>
            <li>2022: Center on Frontiers of Computing Studies (CFCS) Outstanding Student</li>
          </p>
          <p>
            <li>2022: Arawana Scholarship (12000RMB¬•)</li>
          </p>
          <p>
            <li>2022: Zhongying Moral Education Scholarship (4000RMB¬•)</li>
          </p>
          <p>
            <li>2022: Merit Student of Peking University</li>
          </p>
          <p>
            <li>2022: Turing Student Research Forum: Outstanding Presentation Award (1000RMB¬•)</li>
          </p>
          <p>
            <li>2021: National Scholarship (Highest honor for undergraduates in China, awarded to top 1% students, 8000RMB¬•)</li>
          </p>
          <p>
            <li>2021: SenseTime Scholarship (Youngest winner, 20000RMB¬•)</li>
          </p>
          <p>
            <li>2021: Zhongying Moral Education Scholarship (4000RMB¬•)</li>
          </p>
          <p>
            <li>2021: Merit Student of Peking University</li>
          </p>
          <p>
            <li>2021: Ministry of Education Top Talent Program Scholarship (1000RMB¬•)</li>
          </p>
          <p>
            <li>2020: China University Computer Competition - Group Programming LadderCompetition Silver Medalist</li>
          </p>
          <p>
            <li>2019: The 36th Chinese Physics Olympiad(CPHO) Silver Medalist</li>
          </p>
          <p>
            <li>2019: The First Prize of National High School Mathematics Contest, Tianjin, China</li>
          </p>
          
        </td>
      </tr>
  </tbody></table>

  <!-- Miscellaneous -->
  <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <br>
          <heading>Miscellaneous</heading>
          <!-- <br> -->
          <br>
          <!-- <br> -->
        <td style="padding:0px;width:100%;vertical-align:middle">
          
          <p>
            In addition to my studies and research, I have a wide range of interests. 
            <br><li>I love sports and excel in middle-distance runningüèÉ‚Äç‚ôÇÔ∏è, swimmingüèä‚Äç‚ôÇÔ∏è, etc. I am also honored to be awarded the title of <a href="images/sport.png" target="_blank">Excellent Level Athlete (Level 3)</a> at Peking University.</li> 
            <br><li>I am very interested in artü•≥ and have also enrolled in the <em>‚ÄúComputing and Arts‚Äù program</em> by Prof. <a href="https://cfcs.pku.edu.cn/baoquan/" target="_blank">Baoquan Chen</a> and Prof. <a href="https://philpeople.org/profiles/feng-peng" target="_blank">Feng Peng</a> in anticipation of making great art through computing and programming. </li>
            <br><li>I enjoy hip-hopüòé, pianoüéπ, and  paintingüé®, and I have been fortunate enough to have earned the title of  <a href="images/art.png" target="_blank">Artistic Specialists Student</a>.</li>
            <br><li>I have a passion for philanthropyüíñ. I have participated in many charity events, and for my outstanding performance and contributions, I have been honored to receive the <a href="https://youth.pku.edu.cn/tzgg/zytz/355467.htm" target="_blank">Zhongying Moral Education Scholarship</a> for two consecutive years.</li>
            <br><li>To promote computer knowledgeüíª, I became one of the leaders of <a href="https://lcpu.club/wiki/index.php?title=%E9%A6%96%E9%A1%B5" target="_blank">Linux Club of Peking University(LCPU)</a>, helping to popularize computer knowledge with students at Peking University.</li> 
          </p>
        </td>
      </tr>
  </tbody></table>
  
  <!-- Aknowledgements -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <br>
      <br>
      <hr>
        <p style="text-align:center">
          This homepage is designed based on <a href="https://jonbarron.info/">Jon Barron</a>'s website and deployed on <a href="https://pages.github.com/">Github Pages</a>. 
          <br>
          ¬© 2023 Haoran Geng
        </p>
      </td>
      </tr>
  </tbody></table>
 
</body>
</html>
